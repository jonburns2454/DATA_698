rural_urban = case_when(
percent_rural == 100 ~ "Completely Rural",
percent_rural >= 50 & percent_rural < 100 ~ "Mostly Rural",
percent_rural < 50 ~ "Mostly Urban",
TRUE ~ NA_character_
)
)
total_rows <- nrow(final)
missing_counts <- colSums(is.na(final))
missing_percentage <- (missing_counts / total_rows) * 100
missing_table <- data.frame(
Column = names(missing_counts),
MissingCount = missing_counts,
MissingPercentage = round(missing_percentage, 2)
)
missing_table <- missing_table[missing_table$MissingCount > 0, ]
print(missing_table)
describe(final, fast = T)
ggplot(missing_table, aes(x = reorder(Column, -MissingPercentage), y = MissingPercentage)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(
title = "Percentage of Missing Data by Column",
x = "Variable",
y = "Missingness"
) +
theme_minimal()
library(ggplot2)
numeric_columns <- sapply(final, is.numeric)
final_numeric <- final[, numeric_columns]
for (col in names(final_numeric)) {
print(
ggplot(final, aes_string(x = col)) +
geom_histogram(fill = "steelblue", bins = 30, color = "black") +
labs(title = paste("Distribution of", col), x = col, y = "Frequency") +
theme_minimal()
)
}
library(dplyr)
yearly_summary <- final %>%
group_by(year) %>%
summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))
print(yearly_summary)
region_summary <- final %>%
group_by(county.x, year) %>%
summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))
print(region_summary)
library(ggplot2)
ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean, color = county.x)) +
geom_line() +
labs(title = "Yearly Food Insecurity Across New York State Counties", x = "Year", y = "Food Insecurity") +
theme_minimal()
ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean)) +
geom_line() +
labs(title = "Yearly Average Food Insecurity Across New York State", x = "Year", y = "Value") +
theme_minimal()
ggplot(final, aes(x = county.x, y = percent_food_insecure, color = rural_urban)) +
geom_boxplot(fill = "steelblue") +
labs(title = "County Level Comparison of Food Insecurity In New York State", x = "County", y = "Food Insecurity Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
final_csv <- write_csv(final, "C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
# Filter relevant columns (adjust as needed)
df <- data %>%
select(
year, fips, county.x, percent_food_insecure,
percent_unemployed, percent_children_in_poverty,
median_household_income, percent_fair_or_poor_health,
high_school_graduation_rate, rural_urban
) %>%
drop_na(percent_food_insecure)  # Remove rows with missing target
# Create lag features (food insecurity from previous year)
df <- df %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
ungroup()
# Encode 'rural_urban' as numeric
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure
# Set up cross-validation
ctrl <- trainControl(
method = "cv",
number = 10,
verboseIter = TRUE
)
# Define XGBoost tuning grid
xgb_grid <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
# Train the model
xgb_model <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = xgb_grid,
verbosity = 0
)
# Print model summary
print(xgb_model)
# Prepare 2024 data to predict 2025 (assuming 2025 features ≈ 2024)
# (If 2025 features are unavailable, use 2024 as proxy)
X_2025 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure))
# Predict 2025 food insecurity
predictions_2025 <- predict(xgb_model, newdata = X_2025)
# Combine with county names
results <- df %>%
filter(year == 2024) %>%
select(fips, county.x) %>%
mutate(pred_2025_food_insecure = predictions_2025)
# View predictions
print(results)
# Predict 2024 (for validation)
pred_2024 <- predict(xgb_model, newdata = X_test)
# Calculate RMSE
rmse <- sqrt(mean((pred_2024 - y_test)^2))
print(paste("RMSE:", rmse))
# Plot actual vs. predicted
ggplot(data.frame(Actual = y_test, Predicted = pred_2024), aes(Actual, Predicted)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Actual vs. Predicted Food Insecurity (2024)")
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
# Filter relevant columns (adjust as needed)
df <- data %>%
select(where(~ all(!is.na(.)))) %>%
drop_na(percent_food_insecure)  # Remove rows with missing target
# Create lag features (food insecurity from previous year)
df <- df %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
ungroup()
# Encode 'rural_urban' as numeric
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure
View(X_test)
# Create lag features (food insecurity from previous year)
df <- df %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
ungroup()
# Encode 'rural_urban' as numeric
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
# XGBoost Retuning --------------------------------------------------------
# Set up enhanced cross-validation
ctrl <- trainControl(
method = "cv",
number = 10,
verboseIter = TRUE,
savePredictions = "final",
summaryFunction = defaultSummary  # Includes RMSE, R², and MAE
)
# EXPANDED tuning grid for retuning
xgb_grid <- expand.grid(
nrounds = c(100, 150, 200),      # Increased from fixed 100
max_depth = c(4, 6, 8),           # Test shallower and deeper trees
eta = c(0.01, 0.1, 0.3),          # Wider learning rate range
gamma = c(0, 0.1, 0.2),           # New: Regularization
colsample_bytree = c(0.8, 1),     # New: Feature subsampling
min_child_weight = c(1, 3),       # New: Controls overfitting
subsample = c(0.8, 1)             # New: Instance subsampling
)
# Train the RETUNED model with progress tracking
xgb_retuned <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = xgb_grid,
verbosity = 1,                    # Show training progress
metric = "RMSE",                  # Focus optimization on RMSE
importance = TRUE                 # Track variable importance
)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
# Filter relevant columns (adjust as needed)
df <- data %>%
select(where(~ all(!is.na(.)))) %>%
drop_na(percent_food_insecure)  # Remove rows with missing target
# Create lag features (food insecurity from previous year)
df <- df %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
ungroup()
# Encode 'rural_urban' as numeric
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE  # Enable parallel processing
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150),          # Reduced options
max_depth = c(4, 6),            # Most useful range
eta = c(0.05, 0.1),            # Optimal learning rates
gamma = 0,                     # Fixed for simplicity
colsample_bytree = 0.8,        # Fixed subsampling
min_child_weight = 1,          # Fixed
subsample = 0.8                # Fixed
)
# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
xgb_fast <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,                 # Suppress warnings
metric = "RMSE"
)
stopCluster(cl)
# View results
print(xgb_fast)
plot(xgb_fast)
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE  # Enable parallel processing
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200),          # Reduced options
max_depth = c(4, 6, 8),            # Most useful range
eta = c(0.05, 0.1),            # Optimal learning rates
gamma = 0,                     # Fixed for simplicity
colsample_bytree = 0.8,        # Fixed subsampling
min_child_weight = 1,          # Fixed
subsample = 0.8                # Fixed
)
# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
xgb_fast <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,                 # Suppress warnings
metric = "RMSE"
)
stopCluster(cl)
# View results
print(xgb_fast)
plot(xgb_fast)
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE  # Enable parallel processing
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200, 300),          # Reduced options
max_depth = c(4, 6, 8),            # Most useful range
eta = c(0.05, 0.1, .15),            # Optimal learning rates
gamma = 0,                     # Fixed for simplicity
colsample_bytree = 0.8,        # Fixed subsampling
min_child_weight = 1,          # Fixed
subsample = 0.8                # Fixed
)
# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
xgb_fast <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,                 # Suppress warnings
metric = "RMSE"
)
stopCluster(cl)
# View results
print(xgb_fast)
plot(xgb_fast)
# Prepare 2024 data to predict 2025 (assuming 2025 features ≈ 2024)
# (If 2025 features are unavailable, use 2024 as proxy)
X_2025 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure))
# Predict 2025 food insecurity
predictions_2025 <- predict(xgb_model, newdata = X_2025)
# Faster XGBoost Tuning ---------------------------------------------------
# Set up parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE,  # Enable parallel processing
savePredictions = "final"
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200, 300),
max_depth = c(4, 6, 8),
eta = c(0.05, 0.1, 0.15),
gamma = 0,
colsample_bytree = 0.8,
min_child_weight = 1,
subsample = 0.8
)
# Train the optimized model
xgb_optimized <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,
metric = "RMSE"
)
stopCluster(cl)
# View tuning results
print(xgb_optimized)
plot(xgb_optimized)
# Get best model parameters
best_params <- xgb_optimized$bestTune
# Prepare 2025 prediction data with feature engineering
X_2025 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure)) %>%
# Add any feature engineering used in training
mutate(
rural_urban = as.numeric(factor(rural_urban)),
childcare_burden_ratio = child_care_cost/median_household_income
)
# Prepare 2025 prediction data with feature engineering
X_2025 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure)) %>%
# Add any feature engineering used in training
mutate(
rural_urban = as.numeric(factor(rural_urban))
)
# Make predictions using the optimized model
predictions_2025 <- predict(xgb_optimized, newdata = X_2025)
# Create comprehensive results dataframe
results_2025 <- df %>%
filter(year == 2024) %>%
select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
mutate(
pred_2025_food_insecure = predictions_2025,
predicted_change = pred_2025_food_insecure - percent_food_insecure,
risk_category = case_when(
pred_2025_food_insecure > quantile(predictions_2025, 0.75) ~ "High Risk",
pred_2025_food_insecure > quantile(predictions_2025, 0.5) ~ "Medium Risk",
TRUE ~ "Low Risk"
)
) %>%
arrange(desc(pred_2025_food_insecure))  # Sort by highest predicted food insecurity
View(results_2025)
# Visualize top 10 high-risk counties
library(ggplot2)
results_2025 %>%
head(10) %>%
ggplot(aes(x = reorder(county.x, -pred_2025_food_insecure), y = pred_2025_food_insecure)) +
geom_col(fill = "firebrick") +
labs(title = "Top 10 High-Risk Counties for 2025 Food Insecurity",
x = "County",
y = "Predicted Food Insecurity Rate") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
View(X_train)
install.packages('keras')
install.packages('tensorflow')
library(keras)
library(tensorflow)
library(tidyverse)
library(caret)
# Data Preparation --------------------------------------------------------
# Convert data to 3D array format for LSTM [samples, timesteps, features]
create_lstm_data <- function(data, lookback = 3) {
n <- nrow(data)
X <- array(dim = c(n - lookback, lookback, ncol(data)))
y <- vector(length = n - lookback)
for (i in 1:(n - lookback)) {
X[i,,] <- as.matrix(data[i:(i + lookback - 1), ])
y[i] <- data[i + lookback, "percent_food_insecure"]
}
return(list(X = X, y = y))
}
# Normalize data
preprocess_data <- function(data) {
preproc <- preProcess(data, method = c("center", "scale"))
list(
data = predict(preproc, data),
preproc = preproc
)
}
# Prepare data by county
county_data <- df %>%
group_by(fips) %>%
arrange(year) %>%
select(-c(county.x, year)) %>%
nest()
# Apply preprocessing and create LSTM sequences
prepped_data <- county_data %>%
mutate(
preprocessed = map(data, ~preprocess_data(select(., -fips))),
lstm_ready = map(preprocessed, ~create_lstm_data(.$data, lookback = 3))
)
install_tensorflow(version = "2.12.0")
install.packages("keras")
install.packages("tensorflow")
library(keras)
library(tensorflow)
install_tensorflow(version = "2.12.0")
install.packages("tensorflow")
library(tensorflow)
install_tensorflow()
tensorflow::uninstall_tensorflow()
library(keras)
library(tensorflow)
try(remove.packages("keras"), silent = TRUE)
# Install required packages
install.packages(c("reticulate", "tensorflow", "keras"))
# Set up Python environment
library(tensorflow)
install_tensorflow(version = "2.16.1", method = "conda", envname = "r-tensorflow")
