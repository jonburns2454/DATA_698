food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
ungroup()
# recode rural_urban as numeric
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# dropping out the non-important features, including the dependent variable
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure
# cross val
ctrl <- trainControl(
method = "cv",
number = 10,
verboseIter = TRUE
)
# Standard xgboost tuning grid
xgb_grid <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_model <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = xgb_grid,
verbosity = 0
)
print(xgb_model)
# predicting 2024 data
X_2024 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure))
# Predicting 2024 food insecurity
predictions_2024 <- predict(xgb_model, newdata = X_2024)
# bringing in county and fips codes
results <- df %>%
filter(year == 2024) %>%
select(fips, county.x) %>%
mutate(pred_2024_food_insecure = predictions_2024)
# View predictions
print(results)
# Predict 2024 (for validPrediPredir)
pred_2024 <- predict(xgb_model, newdata = X_test)
# Calculate RMSE
rmse <- sqrt(mean((pred_2024 - y_test)^2))
print(paste("RMSE:", rmse))
ggplot(data.frame(Actual = y_test, Predicted = pred_2024), aes(Actual, Predicted)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Actual vs. Predicted Food Insecurity (2024)")
library(caret)
library(Metrics)
test_predictions <- predict(xgb_model, newdata = X_test)
actual_values <- y_test
metrics_table <- data.frame(
RMSE = RMSE(test_predictions, actual_values),
MAPE = mape(actual_values, test_predictions) * 100,
MSE = mean((test_predictions - actual_values)^2),
R2 = R2(test_predictions, actual_values)
)
print(metrics_table)
library(tidyverse)
library(xgboost)
library(caret)
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
df <- data %>%
select(where(~ all(!is.na(.)))) %>%
drop_na(percent_food_insecure)
df <- df %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
select(-food_environment_index) %>%
ungroup()
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban))
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
# Faster XGBoost Tuning
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE  # Enable parallel processing
)
# Tuning grid set
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200, 300),          # Reduced options
max_depth = c(4, 6, 8),            # Most useful range
eta = c(0.05, 0.1, .15),            # Optimal learning rates
gamma = 0,                     # Fixed for simplicity
colsample_bytree = 0.8,        # Fixed subsampling
min_child_weight = 1,          # Fixed
subsample = 0.8                # Fixed
)
# Parallel processing to speed things up
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
xgb_fast <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,
metric = "RMSE"
)
stopCluster(cl)
print(xgb_fast)
plot(xgb_fast)
library(doParallel)
library(Metrics)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE,  # Enable parallel processing
savePredictions = "final"
)
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200, 300),
max_depth = c(4, 6, 8),
eta = c(0.05, 0.1, 0.15),
gamma = 0,
colsample_bytree = 0.8,
min_child_weight = 1,
subsample = 0.8
)
# Train the optimized model
xgb_optimized <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,
metric = "RMSE"
)
stopCluster(cl)
print(xgb_optimized)
plot(xgb_optimized)
# Get best model parameters
best_params <- xgb_optimized$bestTune
# Prepare 2025 prediction data with feature engineering
X_2024 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure)) %>%
# Add any feature engineering used in training
mutate(
rural_urban = as.numeric(factor(rural_urban))
)
# Make predictions using the optimized model
predictions_2024 <- predict(xgb_optimized, newdata = X_2024)
# Create comprehensive results dataframe
results_2024 <- df %>%
filter(year == 2024) %>%
select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
mutate(
pred_2024_food_insecure = predictions_2024,
predicted_change = pred_2024_food_insecure - percent_food_insecure,
risk_category = case_when(
pred_2024_food_insecure > quantile(predictions_2024, 0.75) ~ "High Risk",
pred_2024_food_insecure > quantile(predictions_2024, 0.5) ~ "Medium Risk",
TRUE ~ "Low Risk"
)
) %>%
arrange(desc(pred_2024_food_insecure))  # Sort by highest predicted food insecurity
# Save results with timestamps
write.csv(
results_2024,
file = paste0("food_insecurity_predictions_", format(Sys.Date(), "%Y%m%d"), ".csv"),
row.names = FALSE
)
# Visualize top 10 high-risk counties
library(ggplot2)
results_2024 %>%
head(10) %>%
ggplot(aes(x = reorder(county.x, -pred_2024_food_insecure), y = pred_2024_food_insecure)) +
geom_col(fill = "firebrick") +
labs(title = "Top 10 High-Risk Counties for 2024 Food Insecurity",
x = "County",
y = "Predicted Food Insecurity Rate") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(caret)
library(Metrics)
test_predictions <- predict(xgb_optimized, newdata = X_test)
actual_values <- y_test
metrics_table <- data.frame(
RMSE = RMSE(test_predictions, actual_values),
MAPE = mape(actual_values, test_predictions) * 100,
MSE = mean((test_predictions - actual_values)^2),
R2 = R2(test_predictions, actual_values)
)
print(metrics_table)
library(caret)
perm_imp <- varImp(
xgb_optimized,
scale = TRUE,  # Scale to 0-100
type = "permutation",  # Alternative: "model" for native importance
numPermutations = 30  # Reduce for speed, increase for stability
)
ggplot(perm_imp, aes(x = reorder(rownames(perm_imp), Overall), y = Overall)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(x = "Features", y = "Importance (Permutation)",
title = "Permutation Importance") +
theme_minimal()
perm_imp_plot <- ggplot(perm_imp, aes(x = reorder(rownames(perm_imp), Overall), y = Overall)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(x = "Features", y = "Importance (Permutation)",
title = "Permutation Importance (XGBoost Experiment 3)") +
theme_minimal()
ggsave(
filename = "permutation_importance.png",
plot = perm_imp_plot,
device = "png",
width = 8,
height = 6,
units = "in",
dpi = 300,
bg = "white"
)
library(tidyverse)
library(mice)  # For advanced imputation
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
# This sapply function runs through the data and calculates the missing percentage across all columns
missing_pct <- sapply(data, function(x) mean(is.na(x)) * 100)
# 2. Rules I set (arbitrary) to decide what columns to keep and impute and which ones to drop
cols_to_impute <- names(missing_pct[missing_pct <= 25 & missing_pct > 0])
cols_to_drop <- names(missing_pct[missing_pct > 25])
# 3. Drop columns with 25% or more missingness
data_clean <- data %>%
select(-all_of(cols_to_drop))
# 4.
# Numeric columns: median imputation
num_cols <- data_clean %>%
select(where(is.numeric)) %>%
names() %>%
intersect(cols_to_impute)
data_clean <- data_clean %>%
mutate(across(all_of(num_cols), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Categorical Columns: mode imputation
cat_cols <- data_clean %>%
select(where(is.character) | where(is.factor)) %>%
names() %>%
intersect(cols_to_impute)
get_mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
data_clean <- data_clean %>%
mutate(across(all_of(cat_cols), ~ifelse(is.na(.), get_mode(.), .)))
# 5. Drop rows with >25% missingness in remaining columns
row_missing_pct <- apply(data_clean, 1, function(x) mean(is.na(x)) * 100)
data_final <- data_clean[row_missing_pct <= 25, ]
# Create lag features (food insecurity from previous year)
df <- data_final %>%
arrange(fips, year) %>%
group_by(fips) %>%
mutate(
food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
) %>%
select(-food_environment_index) %>%
ungroup()
# Adding in combination variables
df <- df %>%
mutate(rural_urban = as.factor(rural_urban),
rural_urban = as.numeric(rural_urban),
economic_stress = (
percent_unemployed +
percent_children_in_poverty +
percent_severe_housing_problems
) / 3)
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure
X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduce from 10 folds
verboseIter = TRUE,
allowParallel = TRUE
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150),          # Reduced amount of options for nrounds
max_depth = c(3, 4),            # drop down from 6 and 8 tree depth
eta = c(0.01, 0.05),            # dropped down from 3 set options to 2
gamma = c(0,1),                     # 0-1 to add regularization to help overfitting
colsample_bytree = 0.8,
min_child_weight = 2,          # Increased from 1 to help prevent overfitting
subsample = 0.8                # Fixed
)
# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
xgb_fast <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,                 # Suppress warnings
metric = "RMSE"
)
stopCluster(cl)
# View results
print(xgb_fast)
plot(xgb_fast)
# Faster XGBoost Tuning ---------------------------------------------------
# Set up parallel processing
library(doParallel)
library(Metrics)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
method = "cv",
number = 5,  # Reduced from 10 folds
verboseIter = TRUE,
allowParallel = TRUE,  # Enable parallel processing
savePredictions = "final"
)
# Focused tuning grid
xgb_grid_fast <- expand.grid(
nrounds = c(100, 150, 200, 300),
max_depth = c(4, 6, 8), # Back to 4,6,8 because 3,4 RMSE was undesirable
eta = c(0.05, 0.1, 0.15),
gamma = c(0,1),
colsample_bytree = 0.8,
min_child_weight = 2,
subsample = 0.8
)
# Train the optimized model
xgb_optimized <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl_fast,
tuneGrid = xgb_grid_fast,
verbosity = 0,
metric = "RMSE"
)
stopCluster(cl)
# View tuning results
print(xgb_optimized)
plot(xgb_optimized)
# Get best model parameters
best_params <- xgb_optimized$bestTune
# Prepare 2025 prediction data with feature engineering
X_2024 <- df %>%
filter(year == 2024) %>%
select(-c(year, fips, county.x, percent_food_insecure)) %>%
# Add any feature engineering used in training
mutate(
rural_urban = as.numeric(factor(rural_urban))
)
# Make predictions using the optimized model
predictions_2024 <- predict(xgb_optimized, newdata = X_2024)
# Create comprehensive results dataframe
results_2024 <- df %>%
filter(year == 2024) %>%
select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
mutate(
pred_2024_food_insecure = predictions_2024,
predicted_change = pred_2024_food_insecure - percent_food_insecure,
risk_category = case_when(
pred_2024_food_insecure > quantile(predictions_2024, 0.75) ~ "High Risk",
pred_2024_food_insecure > quantile(predictions_2024, 0.5) ~ "Medium Risk",
TRUE ~ "Low Risk"
)
) %>%
arrange(desc(pred_2024_food_insecure))  # Sort by highest predicted food insecurity
# Save results with timestamps
write.csv(
results_2024,
file = paste0("food_insecurity_predictions_", format(Sys.Date(), "%Y%m%d"), ".csv"),
row.names = FALSE
)
# Visualize top 10 high-risk counties
library(ggplot2)
results_2024 %>%
head(10) %>%
ggplot(aes(x = reorder(county.x, -pred_2024_food_insecure), y = pred_2024_food_insecure)) +
geom_col(fill = "firebrick") +
labs(title = "Top 10 High-Risk Counties for 2024 Food Insecurity",
x = "County",
y = "Predicted Food Insecurity Rate") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(caret)
library(Metrics)
test_predictions <- predict(xgb_optimized, newdata = X_test)
actual_values <- y_test
metrics_table <- data.frame(
RMSE = RMSE(test_predictions, actual_values),
MAPE = mape(actual_values, test_predictions) * 100,
MSE = mean((test_predictions - actual_values)^2),
R2 = R2(test_predictions, actual_values)
)
print(metrics_table)
# SF and Tigris are used to call in geospacial data for mapping purposes
library(sf)
library(tigris)
# Get counties in native projection (no transformation)
ny_counties <- counties(state = "NY", cb = TRUE, year = 2021, class = "sf") %>%
select(fips = GEOID, geometry)
# Filtering out the NY Total Row
results_2024_clean <- results_2024 %>%
filter(fips != 36000) %>%
mutate(fips = as.character(fips))
# Merge and plot the predictions with sf and tigris mapping data
ny_counties %>%
left_join(results_2024_clean, by = "fips") %>%
drop_na(pred_2024_food_insecure) %>%
ggplot() +
geom_sf(aes(fill = pred_2024_food_insecure), color = NA) +
scale_fill_viridis_c("Food Insecurity (%)", option = "inferno", direction = -1) +
labs(title = "New York State 2025 Predicted Food Insecurity by County") +
theme_void()
ny_map <- ny_counties %>%
left_join(results_2024_clean, by = "fips") %>%
drop_na(pred_2024_food_insecure) %>%
ggplot() +
geom_sf(aes(fill = pred_2024_food_insecure), color = NA) +
scale_fill_viridis_c(
"Food Insecurity (%)",
option = "inferno",
direction = -1,
limits = c(0, max(results_2024_clean$pred_2024_food_insecure))
) +
labs(
title = "New York State 2024 Predicted Food Insecurity by County",
caption = "Data: County Health Rankings"
) +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
legend.position = "right",
legend.title = element_text(size = 10)
)
# Save as PNG
ggsave(
filename = "ny_food_insecurity_2025.png",
plot = ny_map,
device = "png",
width = 10,
height = 8,
units = "in",
dpi = 300,
bg = "white"
)
library(caret)
perm_imp <- varImp(
xgb_optimized,
scale = TRUE,  # Scale to 0-100
type = "permutation",  # Alternative: "model" for native importance
numPermutations = 30  # Reduce for speed, increase for stability
)
ggplot(perm_imp, aes(x = reorder(rownames(perm_imp), Overall), y = Overall)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(x = "Features", y = "Importance (Permutation)",
title = "Permutation Importance") +
theme_minimal()
perm_imp_plot <- ggplot(perm_imp, aes(x = reorder(rownames(perm_imp), Overall), y = Overall)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(x = "Features", y = "Importance (Permutation)",
title = "Permutation Importance (XGBoost Experiment 3)") +
theme_minimal()
ggsave(
filename = "permutation_importance.png",
plot = perm_imp_plot,
device = "png",
width = 8,
height = 6,
units = "in",
dpi = 300,
bg = "white"
)
