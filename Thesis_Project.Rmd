---
title: "Thesis Project"
author: "Jonathan Burns"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Packages Used : running list
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
library(psych)
```


# Data Cleaning

```{r}
# Specify the sheets and columns you want

columns_to_extract <- c("year","fips", "state", "county", "percent_food_insecure","percent_frequent_mental_distress",
                        "percent_uninsured_children", "percent_disconnected_youth", "spending_per_pupil","school_funding_adequacy", "high_school_graduation_rate", "median_household_income", "gender_pay_gap", "percent_enrolled_in_free_or_reduced_lunch", "percent_household_income_required_for_child_care_expenses", "percent_households_with_severe_cost_burden", "percent_rural", "percent_65_and_over", "percent_black","percent_not_proficient_in_english", "segregation_index", "percent_disconnected_youth", "food_environment_index", "teen_birth_rate", "percent_fair_or_poor_health", "percent_unemployed", "percent_children_in_single_parent_households", "percent_children_in_poverty", "percent_severe_housing_problems", "percent_completed_high_school","percent_completed_high_school", "percent_low_birthweight")

```

## Pull in data

```{r}
main_25 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_25.csv"
supp_25 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_25.csv"
main_24 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_24.csv"
supp_24 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_24.csv"
main_23 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_23.csv"
supp_23 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_23.csv"
main_22 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_22.csv"
supp_22 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_22.csv"
main_21 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_21.csv"
supp_21 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_21.csv"
main_20 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_20.csv"
supp_20 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_20.csv"
```


```{r}
clean_and_select <- function(file_path, columns_to_extract) {
  data <- read.csv(file_path, check.names = FALSE) %>%
    janitor::clean_names() %>%
    select(any_of(columns_to_extract))
  return(data)
}
m_25 <- clean_and_select(main_25, columns_to_extract)
s_25 <- clean_and_select(supp_25, columns_to_extract)

m_24 <- clean_and_select(main_24, columns_to_extract)
s_24 <- clean_and_select(supp_24, columns_to_extract)

m_23 <- clean_and_select(main_23, columns_to_extract)
s_23 <- clean_and_select(supp_23, columns_to_extract)

m_22 <- clean_and_select(main_22, columns_to_extract)
s_22 <- clean_and_select(supp_22, columns_to_extract)

m_21 <- clean_and_select(main_21, columns_to_extract)
s_21 <- clean_and_select(supp_21, columns_to_extract)

m_20 <- clean_and_select(main_20, columns_to_extract)
s_20 <- clean_and_select(supp_20, columns_to_extract)

```

## Fix Missing Total and join
```{r}

f_25 <- m_25 %>% 
  left_join(s_25, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_24 <- m_24 %>% 
  left_join(s_24, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_23 <- m_23 %>% 
  left_join(s_23, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_22 <- m_22 %>% 
  left_join(s_22, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_21 <- m_21 %>% 
  left_join(s_21, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_20 <- m_20 %>% 
  left_join(s_20, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)
```

```{r}
final <- bind_rows(f_25, f_24, f_23, f_22, f_21, f_20)
final_data_clean <- final %>%
  select(where(~ !any(is.na(.))))
```

# Rural | Urban Category
```{r}
final <- final %>%
  mutate(
    rural_urban = case_when(
      percent_rural == 100 ~ "Completely Rural",
      percent_rural >= 50 & percent_rural < 100 ~ "Mostly Rural",
      percent_rural < 50 ~ "Mostly Urban",
      TRUE ~ NA_character_ 
    )
  )
    
```

## Dropping NA Columns vs Imputing

```{r}

total_rows <- nrow(final)


missing_counts <- colSums(is.na(final))

missing_percentage <- (missing_counts / total_rows) * 100


missing_table <- data.frame(
  Column = names(missing_counts),
  MissingCount = missing_counts,
  MissingPercentage = round(missing_percentage, 2) 
)

missing_table <- missing_table[missing_table$MissingCount > 0, ]

print(missing_table)
```

# EDA

## Summary Statistics

```{r}
describe(final, fast = T)
```




## Missingness Visualized

```{r}
ggplot(missing_table, aes(x = reorder(Column, -MissingPercentage), y = MissingPercentage)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() + 
  labs(
    title = "Percentage of Missing Data by Column",
    x = "Variable",
    y = "Missingness"
  ) +
  theme_minimal()
```

##Numeric Distribution

```{r}
library(ggplot2)
numeric_columns <- sapply(final, is.numeric)
final_numeric <- final[, numeric_columns]

for (col in names(final_numeric)) {
  print(
    ggplot(final, aes_string(x = col)) +
      geom_histogram(fill = "steelblue", bins = 30, color = "black") +
      labs(title = paste("Distribution of", col), x = col, y = "Frequency") +
      theme_minimal()
  )
}
```


## By Year:

```{r}
library(dplyr)

yearly_summary <- final %>%
  group_by(year) %>%
  summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))

print(yearly_summary)
```

## By County
```{r}
region_summary <- final %>%
  group_by(county.x, year) %>%
  summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))

print(region_summary)
```



```{r}
library(ggplot2)

ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean, color = county.x)) +
  geom_line() +
  labs(title = "Yearly Food Insecurity Across New York State Counties", x = "Year", y = "Food Insecurity") +
  theme_minimal()
```

```{r}

ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean)) +
  geom_line() +
  labs(title = "Yearly Average Food Insecurity Across New York State", x = "Year", y = "Value") +
  theme_minimal()
```

```{r}

ggplot(final, aes(x = county.x, y = percent_food_insecure, color = rural_urban)) +
  geom_boxplot(fill = "steelblue") +
  labs(title = "County Level Comparison of Food Insecurity In New York State", x = "County", y = "Food Insecurity Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

```{r}
final_csv <- write_csv(final, "C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
```


# Model Testing:

## XGBoost 1:
```{r}
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)

# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

# Filter relevant columns (adjust as needed)
df <- data %>%
  select(
    year, fips, county.x, percent_food_insecure,
    percent_unemployed, percent_children_in_poverty,
    median_household_income, percent_fair_or_poor_health,
    high_school_graduation_rate, rural_urban
  ) %>%
  drop_na(percent_food_insecure)  # Remove rows with missing target
```

```{r}
# Create lag features (food insecurity from previous year)
df <- df %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
    food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
  ) %>%
  ungroup()

# Encode 'rural_urban' as numeric
df <- df %>%
  mutate(rural_urban = as.factor(rural_urban),
         rural_urban = as.numeric(rural_urban))
```

```{r}
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure
```

```{r}
# Set up cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10,
  verboseIter = TRUE
)

# Define XGBoost tuning grid
xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Train the model
xgb_model <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  verbosity = 0
)

# Print model summary
print(xgb_model)
```

```{r}
# Prepare 2024 data to predict 2025 (assuming 2025 features â‰ˆ 2024)
# (If 2025 features are unavailable, use 2024 as proxy)
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure))

# Predict 2025 food insecurity
predictions_2025 <- predict(xgb_model, newdata = X_2025)

# Combine with county names
results <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x) %>%
  mutate(pred_2025_food_insecure = predictions_2025)

# View predictions
print(results)
```

```{r}
# Predict 2024 (for validation)
pred_2024 <- predict(xgb_model, newdata = X_test)

# Calculate RMSE
rmse <- sqrt(mean((pred_2024 - y_test)^2))
print(paste("RMSE:", rmse))

# Plot actual vs. predicted
ggplot(data.frame(Actual = y_test, Predicted = pred_2024), aes(Actual, Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs. Predicted Food Insecurity (2024)")
```

## XGBoost 2:

```{r}
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)

# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

# Filter relevant columns (adjust as needed)
df <- data %>%
  select(where(~ all(!is.na(.)))) %>%
  drop_na(percent_food_insecure)  # Remove rows with missing target
```

```{r}
# Create lag features (food insecurity from previous year)
df <- df %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
    food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
  ) %>%
  ungroup()

# Encode 'rural_urban' as numeric
df <- df %>%
  mutate(rural_urban = as.factor(rural_urban),
         rural_urban = as.numeric(rural_urban))
```

```{r}
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
```

```{r warning=FALSE}
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE  # Enable parallel processing
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150, 200, 300),          # Reduced options
  max_depth = c(4, 6, 8),            # Most useful range
  eta = c(0.05, 0.1, .15),            # Optimal learning rates
  gamma = 0,                     # Fixed for simplicity
  colsample_bytree = 0.8,        # Fixed subsampling
  min_child_weight = 1,          # Fixed
  subsample = 0.8                # Fixed
)

# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

xgb_fast <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,                 # Suppress warnings
  metric = "RMSE"
)

stopCluster(cl)

# View results
print(xgb_fast)
plot(xgb_fast)
```


```{r}
# Faster XGBoost Tuning ---------------------------------------------------
# Set up parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE,  # Enable parallel processing
  savePredictions = "final"
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150, 200, 300),
  max_depth = c(4, 6, 8),
  eta = c(0.05, 0.1, 0.15),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

# Train the optimized model
xgb_optimized <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,
  metric = "RMSE"
)

stopCluster(cl)

# View tuning results
print(xgb_optimized)
plot(xgb_optimized)

# Get best model parameters
best_params <- xgb_optimized$bestTune

# Prepare 2025 prediction data with feature engineering
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure)) %>%
  # Add any feature engineering used in training
  mutate(
    rural_urban = as.numeric(factor(rural_urban))
  )

# Make predictions using the optimized model
predictions_2025 <- predict(xgb_optimized, newdata = X_2025)

# Create comprehensive results dataframe
results_2025 <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
  mutate(
    pred_2025_food_insecure = predictions_2025,
    predicted_change = pred_2025_food_insecure - percent_food_insecure,
    risk_category = case_when(
      pred_2025_food_insecure > quantile(predictions_2025, 0.75) ~ "High Risk",
      pred_2025_food_insecure > quantile(predictions_2025, 0.5) ~ "Medium Risk",
      TRUE ~ "Low Risk"
    )
  ) %>%
  arrange(desc(pred_2025_food_insecure))  # Sort by highest predicted food insecurity

# Save results with timestamps
write.csv(
  results_2025,
  file = paste0("food_insecurity_predictions_", format(Sys.Date(), "%Y%m%d"), ".csv"),
  row.names = FALSE
)

# Visualize top 10 high-risk counties
library(ggplot2)
results_2025 %>%
  head(10) %>%
  ggplot(aes(x = reorder(county.x, -pred_2025_food_insecure), y = pred_2025_food_insecure)) +
  geom_col(fill = "firebrick") +
  labs(title = "Top 10 High-Risk Counties for 2025 Food Insecurity",
       x = "County",
       y = "Predicted Food Insecurity Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Neural Network 1:

```{r}
# Load required libraries
library(tidyverse)
library(keras)
library(tensorflow)
library(caret)
library(recipes)

# Set random seed for reproducibility
set.seed(42)
tensorflow::set_random_seed(42)

# Read and prepare data (same as your XGBoost code)
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

df <- data %>%
  select(
    year, fips, county.x, percent_food_insecure,
    percent_unemployed, percent_children_in_poverty,
    median_household_income, percent_fair_or_poor_health,
    high_school_graduation_rate, rural_urban
  ) %>%
  drop_na(percent_food_insecure)

# Create lag features
df <- df %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),
    food_insecure_lag2 = lag(percent_food_insecure, 2)
  ) %>%
  ungroup() %>%
  mutate(rural_urban = as.numeric(as.factor(rural_urban)))

# Split data
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Prepare features and target
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure

# Preprocessing with recipes (similar to XGBoost's handling of numeric features)
preprocessor <- recipe(~ ., data = X_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep()

X_train_processed <- bake(preprocessor, new_data = X_train)
X_test_processed <- bake(preprocessor, new_data = X_test)

# Convert to matrix format for Keras
X_train_matrix <- as.matrix(X_train_processed)
X_test_matrix <- as.matrix(X_test_processed)

build_nn_model <- function(input_shape) {
  model <- keras_model_sequential() %>%
    # Input layer
    layer_dense(units = 64, activation = "relu", input_shape = input_shape) %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    
    # Hidden layers
    layer_dense(units = 64, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    
    layer_dense(units = 64, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    
    # Output layer
    layer_dense(units = 1, activation = "linear")
  
  # Compile model
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_adam(learning_rate = 0.01),
    metrics = c("mae")
  )
  
  return(model)
}

# Create and train model - corrected workflow
model <- build_nn_model(input_shape = ncol(X_train_matrix))

# Early stopping callback
early_stopping <- callback_early_stopping(
  monitor = "val_loss",
  patience = 10,
  restore_best_weights = TRUE
)

# Train model
history <- model %>% fit(
  x = X_train_matrix,
  y = y_train,
  epochs = 200,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(early_stopping),
  verbose = 1
)

# Plot training history
plot(history)

# Evaluate on test set
test_performance <- model %>% evaluate(X_test_matrix, y_test)
print(paste("Test MSE:", test_performance[1]))
print(paste("Test MAE:", test_performance[2]))

# Predict 2025 values
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure))

X_2025_processed <- bake(preprocessor, new_data = X_2025)
X_2025_matrix <- as.matrix(X_2025_processed)

predictions_2025 <- model %>% predict(X_2025_matrix)

# Prepare results (same format as XGBoost output)
results_nn <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x) %>%
  mutate(pred_2025_food_insecure = predictions_2025[,1])

# Compare with actual 2024 values for validation
results_nn <- results_nn %>%
  left_join(test %>% select(fips, actual_2024 = percent_food_insecure), by = "fips")

# Calculate RMSE (to match XGBoost metric)
rmse_nn <- sqrt(mean((results_nn$pred_2025_food_insecure - results_nn$actual_2024)^2))
print(paste("Neural Network RMSE:", rmse_nn))

# Plot actual vs predicted
ggplot(results_nn, aes(x = actual_2024, y = pred_2025_food_insecure)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Neural Network: Actual vs Predicted Food Insecurity",
       x = "Actual 2024 Values",
       y = "Predicted 2025 Values")
```

```{r}
# Load required libraries
library(tidyverse)
library(keras)
library(tensorflow)
library(caret)
library(recipes)

# Verify installation
tensorflow::tf_config()
keras::is_keras_available()

# Set random seeds for reproducibility
set.seed(42)
tensorflow::set_random_seed(42)
tf$compat$v1$set_random_seed(42)

# Build neural network model - revised version
build_nn_model <- function(input_shape) {
  model <- keras_model_sequential(name = "xgboost_style_nn") %>%
    # Input layer
    layer_dense(units = 64, activation = "relu", input_shape = input_shape, 
               kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    
    # Hidden layers with residual-like connections
    layer_dense(units = 64, activation = "relu",
               kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)) %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    
    # Output layer
    layer_dense(units = 1, activation = "linear")
  
  # Custom optimizer configuration
  optimizer <- optimizer_adam(
    learning_rate = 0.01,
    beta_1 = 0.9,
    beta_2 = 0.999,
    epsilon = 1e-07
  )
  
  # Compile model
  model %>% compile(
    loss = "mse",
    optimizer = optimizer,
    metrics = c("mae", "mse")
  )
  
  return(model)
}

# Create model
model <- build_nn_model(input_shape = ncol(X_train_matrix))

# Print model summary
summary(model)

# Configure callbacks
callbacks <- list(
  callback_early_stopping(
    monitor = "val_loss",
    patience = 10,
    restore_best_weights = TRUE
  ),
  callback_reduce_lr_on_plateau(
    monitor = "val_loss",
    factor = 0.1,
    patience = 5
  )
)

# Train model with proper data conversion
history <- model %>% fit(
  x = array(X_train_matrix, dim = dim(X_train_matrix)),
  y = array(y_train, dim = length(y_train)),
  epochs = 200,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = callbacks,
  verbose = 1
)

# Plot training history
plot(history)

# Evaluate model
evaluation <- model %>% evaluate(
  x = array(X_test_matrix, dim = dim(X_test_matrix)),
  y = array(y_test, dim = length(y_test))
)

print(paste("Test MSE:", evaluation["mse"]))
print(paste("Test MAE:", evaluation["mae"]))
```




