---
title: "Thesis Project"
author: "Jonathan Burns"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Packages Used : running list
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
library(psych)
```


# Data Cleaning

```{r}
# Specify the sheets and columns you want

columns_to_extract <- c("year","fips", "state", "county", "percent_food_insecure","percent_frequent_mental_distress",
                        "percent_uninsured_children", "percent_disconnected_youth", "spending_per_pupil","school_funding_adequacy", "high_school_graduation_rate", "median_household_income", "gender_pay_gap", "percent_enrolled_in_free_or_reduced_lunch", "percent_household_income_required_for_child_care_expenses", "percent_households_with_severe_cost_burden", "percent_rural", "percent_65_and_over", "percent_black","percent_not_proficient_in_english", "segregation_index", "percent_disconnected_youth", "food_environment_index", "teen_birth_rate", "percent_fair_or_poor_health", "percent_unemployed", "percent_children_in_single_parent_households", "percent_children_in_poverty", "percent_severe_housing_problems", "percent_completed_high_school","percent_completed_high_school", "percent_low_birthweight")

```

## Pull in data

```{r}
main_25 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_25.csv"
supp_25 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_25.csv"
main_24 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_24.csv"
supp_24 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_24.csv"
main_23 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_23.csv"
supp_23 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_23.csv"
main_22 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_22.csv"
supp_22 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_22.csv"
main_21 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_21.csv"
supp_21 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_21.csv"
main_20 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/main_20.csv"
supp_20 <- "https://raw.githubusercontent.com/jonburns2454/DATA_698/refs/heads/main/DATA/supp_20.csv"
```


```{r}
clean_and_select <- function(file_path, columns_to_extract) {
  data <- read.csv(file_path, check.names = FALSE) %>%
    janitor::clean_names() %>%
    select(any_of(columns_to_extract))
  return(data)
}
m_25 <- clean_and_select(main_25, columns_to_extract)
s_25 <- clean_and_select(supp_25, columns_to_extract)

m_24 <- clean_and_select(main_24, columns_to_extract)
s_24 <- clean_and_select(supp_24, columns_to_extract)

m_23 <- clean_and_select(main_23, columns_to_extract)
s_23 <- clean_and_select(supp_23, columns_to_extract)

m_22 <- clean_and_select(main_22, columns_to_extract)
s_22 <- clean_and_select(supp_22, columns_to_extract)

m_21 <- clean_and_select(main_21, columns_to_extract)
s_21 <- clean_and_select(supp_21, columns_to_extract)

m_20 <- clean_and_select(main_20, columns_to_extract)
s_20 <- clean_and_select(supp_20, columns_to_extract)

```

## Fix Missing Total and join
```{r}

f_25 <- m_25 %>% 
  left_join(s_25, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_24 <- m_24 %>% 
  left_join(s_24, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_23 <- m_23 %>% 
  left_join(s_23, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_22 <- m_22 %>% 
  left_join(s_22, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_21 <- m_21 %>% 
  left_join(s_21, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)

f_20 <- m_20 %>% 
  left_join(s_20, by = c("year", "fips")) %>% 
  select(-state.y, -county.y)
```

```{r}
final <- bind_rows(f_25, f_24, f_23, f_22, f_21, f_20)
final_data_clean <- final %>%
  select(where(~ !any(is.na(.))))
```

# Rural | Urban Category
```{r}
final <- final %>%
  mutate(
    rural_urban = case_when(
      percent_rural == 100 ~ "Completely Rural",
      percent_rural >= 50 & percent_rural < 100 ~ "Mostly Rural",
      percent_rural < 50 ~ "Mostly Urban",
      TRUE ~ NA_character_ 
    )
  )
    
```

## Dropping NA Columns vs Imputing

```{r}

total_rows <- nrow(final)


missing_counts <- colSums(is.na(final))

missing_percentage <- (missing_counts / total_rows) * 100


missing_table <- data.frame(
  Column = names(missing_counts),
  MissingCount = missing_counts,
  MissingPercentage = round(missing_percentage, 2) 
)

missing_table <- missing_table[missing_table$MissingCount > 0, ]

print(missing_table)
```

# EDA

## Summary Statistics

```{r}
describe(final, fast = T)
```




## Missingness Visualized

```{r}
ggplot(missing_table, aes(x = reorder(Column, -MissingPercentage), y = MissingPercentage)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() + 
  labs(
    title = "Percentage of Missing Data by Column",
    x = "Variable",
    y = "Missingness"
  ) +
  theme_minimal()
```

##Numeric Distribution

```{r}
library(ggplot2)
numeric_columns <- sapply(final, is.numeric)
final_numeric <- final[, numeric_columns]

for (col in names(final_numeric)) {
  print(
    ggplot(final, aes_string(x = col)) +
      geom_histogram(fill = "steelblue", bins = 30, color = "black") +
      labs(title = paste("Distribution of", col), x = col, y = "Frequency") +
      theme_minimal()
  )
}
```


## By Year:

```{r}
library(dplyr)

yearly_summary <- final %>%
  group_by(year) %>%
  summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))

print(yearly_summary)
```

## By County
```{r}
region_summary <- final %>%
  group_by(county.x, year) %>%
  summarise(across(where(is.numeric), list(mean = mean, median = median, sd = sd), na.rm = TRUE))

print(region_summary)
```



```{r}
library(ggplot2)

ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean, color = county.x)) +
  geom_line() +
  labs(title = "Yearly Food Insecurity Across New York State Counties", x = "Year", y = "Food Insecurity") +
  theme_minimal()
```

```{r}

ggplot(region_summary, aes(x = year, y = percent_food_insecure_mean)) +
  geom_line() +
  labs(title = "Yearly Average Food Insecurity Across New York State", x = "Year", y = "Value") +
  theme_minimal()
```

```{r}

ggplot(final, aes(x = county.x, y = percent_food_insecure, color = rural_urban)) +
  geom_boxplot(fill = "steelblue") +
  labs(title = "County Level Comparison of Food Insecurity In New York State", x = "County", y = "Food Insecurity Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```

```{r}
final_csv <- write_csv(final, "C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")
```


# Model Testing:

## XGBoost 1:
```{r}
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)

# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

# Filter relevant columns (adjust as needed)
df <- data %>%
  select(
    year, fips, county.x, percent_food_insecure,
    percent_unemployed, percent_children_in_poverty,
    median_household_income, percent_fair_or_poor_health,
    high_school_graduation_rate, rural_urban
  ) %>%
  drop_na(percent_food_insecure)  # Remove rows with missing target
```

```{r}
# Create lag features (food insecurity from previous year)
df <- df %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
    food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
  ) %>%
  ungroup()

# Encode 'rural_urban' as numeric
df <- df %>%
  mutate(rural_urban = as.factor(rural_urban),
         rural_urban = as.numeric(rural_urban))
```

```{r}
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure))
y_test <- test$percent_food_insecure
```

```{r}
# Set up cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10,
  verboseIter = TRUE
)

# Define XGBoost tuning grid
xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Train the model
xgb_model <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = xgb_grid,
  verbosity = 0
)

# Print model summary
print(xgb_model)
```

```{r}
# Prepare 2024 data to predict 2025 (assuming 2025 features â‰ˆ 2024)
# (If 2025 features are unavailable, use 2024 as proxy)
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure))

# Predict 2025 food insecurity
predictions_2025 <- predict(xgb_model, newdata = X_2025)

# Combine with county names
results <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x) %>%
  mutate(pred_2025_food_insecure = predictions_2025)

# View predictions
print(results)
```

```{r}
# Predict 2024 (for validation)
pred_2024 <- predict(xgb_model, newdata = X_test)

# Calculate RMSE
rmse <- sqrt(mean((pred_2024 - y_test)^2))
print(paste("RMSE:", rmse))

# Plot actual vs. predicted
ggplot(data.frame(Actual = y_test, Predicted = pred_2024), aes(Actual, Predicted)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs. Predicted Food Insecurity (2024)")
```

## XGBoost 2:

```{r}
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)

# Read the CSV
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

# Filter relevant columns (adjust as needed)
df <- data %>%
  select(where(~ all(!is.na(.)))) %>%
  drop_na(percent_food_insecure)  # Remove rows with missing target
```

```{r}
# Create lag features (food insecurity from previous year)
df <- df %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
    food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
  ) %>%
  ungroup()

# Encode 'rural_urban' as numeric
df <- df %>%
  mutate(rural_urban = as.factor(rural_urban),
         rural_urban = as.numeric(rural_urban))
```

```{r}
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
```

```{r warning=FALSE}
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE  # Enable parallel processing
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150, 200, 300),          # Reduced options
  max_depth = c(4, 6, 8),            # Most useful range
  eta = c(0.05, 0.1, .15),            # Optimal learning rates
  gamma = 0,                     # Fixed for simplicity
  colsample_bytree = 0.8,        # Fixed subsampling
  min_child_weight = 1,          # Fixed
  subsample = 0.8                # Fixed
)

# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

xgb_fast <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,                 # Suppress warnings
  metric = "RMSE"
)

stopCluster(cl)

# View results
print(xgb_fast)
plot(xgb_fast)
```


```{r}
# Faster XGBoost Tuning ---------------------------------------------------
# Set up parallel processing
library(doParallel)
library(Metrics)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE,  # Enable parallel processing
  savePredictions = "final"
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150, 200, 300),
  max_depth = c(4, 6, 8),
  eta = c(0.05, 0.1, 0.15),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

# Train the optimized model
xgb_optimized <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,
  metric = "RMSE"
)

stopCluster(cl)

# View tuning results
print(xgb_optimized)
plot(xgb_optimized)

# Get best model parameters
best_params <- xgb_optimized$bestTune

# Prepare 2025 prediction data with feature engineering
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure)) %>%
  # Add any feature engineering used in training
  mutate(
    rural_urban = as.numeric(factor(rural_urban))
  )

# Make predictions using the optimized model
predictions_2025 <- predict(xgb_optimized, newdata = X_2025)

# Create comprehensive results dataframe
results_2025 <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
  mutate(
    pred_2025_food_insecure = predictions_2025,
    predicted_change = pred_2025_food_insecure - percent_food_insecure,
    risk_category = case_when(
      pred_2025_food_insecure > quantile(predictions_2025, 0.75) ~ "High Risk",
      pred_2025_food_insecure > quantile(predictions_2025, 0.5) ~ "Medium Risk",
      TRUE ~ "Low Risk"
    )
  ) %>%
  arrange(desc(pred_2025_food_insecure))  # Sort by highest predicted food insecurity

# Save results with timestamps
write.csv(
  results_2025,
  file = paste0("food_insecurity_predictions_", format(Sys.Date(), "%Y%m%d"), ".csv"),
  row.names = FALSE
)

# Visualize top 10 high-risk counties
library(ggplot2)
results_2025 %>%
  head(10) %>%
  ggplot(aes(x = reorder(county.x, -pred_2025_food_insecure), y = pred_2025_food_insecure)) +
  geom_col(fill = "firebrick") +
  labs(title = "Top 10 High-Risk Counties for 2025 Food Insecurity",
       x = "County",
       y = "Predicted Food Insecurity Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}

library(caret)
library(Metrics)

test_predictions <- predict(xgb_optimized, newdata = X_test)
actual_values <- y_test 

metrics_table <- data.frame(
  RMSE = RMSE(test_predictions, actual_values),
  MAPE = mape(actual_values, test_predictions) * 100,
  MSE = mean((test_predictions - actual_values)^2),
  R2 = R2(test_predictions, actual_values)
)

print(metrics_table)
```

## XGBoost 3:

This XGBoost model will attempt to do four things. Make sure the lag variables are actually backward-looking by adjusting how they are set. Address overfitting by increasing regularization and increase early stopping. Lastly, add in more interaction terms

```{r}
library(tidyverse)
library(mice)  # For advanced imputation

# Load data
data <- read.csv("C:\\Users\\jashb\\OneDrive\\Documents\\Masters Data Science\\Spring 2025\\DATA 698\\Masters Project\\final_data.csv")

# 1. Calculate missingness percentages
missing_pct <- sapply(data, function(x) mean(is.na(x)) * 100)

# 2. Identify columns to impute (â‰¤25% missing) vs. drop (>25%)
cols_to_impute <- names(missing_pct[missing_pct <= 25 & missing_pct > 0])
cols_to_drop <- names(missing_pct[missing_pct > 25])

# 3. Drop columns with >25% missingness
data_clean <- data %>% 
  select(-all_of(cols_to_drop))

# 4. Impute remaining missing values (â‰¤25%)
# For numeric columns: median imputation
num_cols <- data_clean %>% 
  select(where(is.numeric)) %>% 
  names() %>% 
  intersect(cols_to_impute)

data_clean <- data_clean %>%
  mutate(across(all_of(num_cols), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))

# For categorical columns: mode imputation
cat_cols <- data_clean %>% 
  select(where(is.character) | where(is.factor)) %>% 
  names() %>% 
  intersect(cols_to_impute)

get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

data_clean <- data_clean %>%
  mutate(across(all_of(cat_cols), ~ifelse(is.na(.), get_mode(.), .)))

# 5. Drop rows with >25% missingness in remaining columns
row_missing_pct <- apply(data_clean, 1, function(x) mean(is.na(x)) * 100)
data_final <- data_clean[row_missing_pct <= 25, ]

```

```{r}
# Create lag features (food insecurity from previous year)
df <- data_final %>%
  arrange(fips, year) %>% 
  group_by(fips) %>%
  mutate(
    food_insecure_lag1 = lag(percent_food_insecure, 1),  # Previous year
    food_insecure_lag2 = lag(percent_food_insecure, 2)   # Two years back
  ) %>%
  ungroup()

# Encode 'rural_urban' as numeric
df <- df %>%
  mutate(rural_urban = as.factor(rural_urban),
         rural_urban = as.numeric(rural_urban),
         food_risk_score = (
      (percent_food_insecure * 0.4) + 
      (percent_enrolled_in_free_or_reduced_lunch * 0.3) + 
      (1 - food_environment_index * 0.3)
    ),
    economic_stress = (
      percent_unemployed + 
      percent_children_in_poverty + 
      percent_severe_housing_problems
    ) / 3)
```

```{r}
train <- df %>% filter(year < 2024)
test <- df %>% filter(year == 2024)

# Separate features (X) and target (y)
X_train <- train %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_train <- train$percent_food_insecure

X_test <- test %>% select(-c(year, fips, county.x, percent_food_insecure, state.x))
y_test <- test$percent_food_insecure
```

```{r warning=FALSE}
# Faster XGBoost Tuning ---------------------------------------------------
# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE  # Enable parallel processing
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150),          # Reduced options
  max_depth = c(3, 4),            # drop down from 6 and 8
  eta = c(0.01, 0.05),            # dropped down from 3 set options
  gamma = c(0,1),                     # 0-1 to add regularization
  colsample_bytree = 0.8,        # 
  min_child_weight = 2,          # Increased from 1 to help prevent overfitting
  subsample = 0.8                # Fixed
)

# Train with parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

xgb_fast <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,                 # Suppress warnings
  metric = "RMSE"
)

stopCluster(cl)

# View results
print(xgb_fast)
plot(xgb_fast)
```

```{r}

# Faster XGBoost Tuning ---------------------------------------------------
# Set up parallel processing
library(doParallel)
library(Metrics)
cl <- makePSOCKcluster(4)  # Use 4 cores
registerDoParallel(cl)

# Reduced tuning grid with most impactful parameters
ctrl_fast <- trainControl(
  method = "cv",
  number = 5,  # Reduced from 10 folds
  verboseIter = TRUE,
  allowParallel = TRUE,  # Enable parallel processing
  savePredictions = "final"
)

# Focused tuning grid
xgb_grid_fast <- expand.grid(
  nrounds = c(100, 150, 200, 300),
  max_depth = c(4, 6, 8),
  eta = c(0.05, 0.1, 0.15),
  gamma = c(0,1),
  colsample_bytree = 0.8,
  min_child_weight = 2,
  subsample = 0.8
)

# Train the optimized model
xgb_optimized <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl_fast,
  tuneGrid = xgb_grid_fast,
  verbosity = 0,
  metric = "RMSE"
)

stopCluster(cl)

# View tuning results
print(xgb_optimized)
plot(xgb_optimized)

# Get best model parameters
best_params <- xgb_optimized$bestTune

# Prepare 2025 prediction data with feature engineering
X_2025 <- df %>% 
  filter(year == 2024) %>%
  select(-c(year, fips, county.x, percent_food_insecure)) %>%
  # Add any feature engineering used in training
  mutate(
    rural_urban = as.numeric(factor(rural_urban))
  )

# Make predictions using the optimized model
predictions_2025 <- predict(xgb_optimized, newdata = X_2025)

# Create comprehensive results dataframe
results_2025 <- df %>%
  filter(year == 2024) %>%
  select(fips, county.x, percent_food_insecure) %>%  # Include current food insecurity
  mutate(
    pred_2025_food_insecure = predictions_2025,
    predicted_change = pred_2025_food_insecure - percent_food_insecure,
    risk_category = case_when(
      pred_2025_food_insecure > quantile(predictions_2025, 0.75) ~ "High Risk",
      pred_2025_food_insecure > quantile(predictions_2025, 0.5) ~ "Medium Risk",
      TRUE ~ "Low Risk"
    )
  ) %>%
  arrange(desc(pred_2025_food_insecure))  # Sort by highest predicted food insecurity

# Save results with timestamps
write.csv(
  results_2025,
  file = paste0("food_insecurity_predictions_", format(Sys.Date(), "%Y%m%d"), ".csv"),
  row.names = FALSE
)

# Visualize top 10 high-risk counties
library(ggplot2)
results_2025 %>%
  head(10) %>%
  ggplot(aes(x = reorder(county.x, -pred_2025_food_insecure), y = pred_2025_food_insecure)) +
  geom_col(fill = "firebrick") +
  labs(title = "Top 10 High-Risk Counties for 2025 Food Insecurity",
       x = "County",
       y = "Predicted Food Insecurity Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
library(caret)
library(Metrics)

test_predictions <- predict(xgb_optimized, newdata = X_test)
actual_values <- y_test 

metrics_table <- data.frame(
  RMSE = RMSE(test_predictions, actual_values),
  MAPE = mape(actual_values, test_predictions) * 100,
  MSE = mean((test_predictions - actual_values)^2),
  R2 = R2(test_predictions, actual_values)
)

print(metrics_table)
```


# Mapping XGBoosts Best Performing Model:

```{r}
# 3. Use this SAFE approach that avoids CRS transformations:
library(sf)
library(tigris)
library(ggplot2)

# Get counties in native projection (no transformation)
ny_counties <- counties(state = "NY", cb = TRUE, year = 2021, class = "sf") %>% 
  select(fips = GEOID, geometry)

# Clean your data
results_2025_clean <- results_2025 %>%
  filter(fips != 36000) %>%
  mutate(fips = as.character(fips))

# Merge and plot
ny_counties %>%
  left_join(results_2025_clean, by = "fips") %>%
  drop_na(pred_2025_food_insecure) %>%
  ggplot() +
  geom_sf(aes(fill = pred_2025_food_insecure), color = NA) +
  scale_fill_viridis_c("Food Insecurity (%)", option = "inferno") +
  labs(title = "2025 Predicted Food Insecurity by County") +
  theme_void()
```



